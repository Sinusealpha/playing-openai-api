{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTn+d2YwclNbJM5MjmmvqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sinusealpha/playing-openai-api/blob/main/step1_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eHEbdIlGgjVW"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "# it is from huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) the simplest way to work with the transformers package:**"
      ],
      "metadata": {
        "id": "zps75OzZjtQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", device = 'cuda')\n",
        "classifier(\"i'm neither happy nor bored to work in the gc space. i'm feeling somehow good.\")\n",
        "# this text is just an example, you can edit it."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmJuEyV7goil",
        "outputId": "6bd8afbf-39f7-4564-f84e-3a41ebf17d7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9994269609451294}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) we can use tokenizer and model seprately:**"
      ],
      "metadata": {
        "id": "YhrN4BjVj-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModel\n",
        "# we can use TFAutoModel to use TensorFlow as BackEnd of transformers module\n",
        "\n",
        "# bert is here to encode texts. it receives a paragraph and give us a 512 dimentional vector to represent the text.\n",
        "# this process is called \"embedding\" and we are gonna use it in the RAG and VectorDBs.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "# be careful, all pre-training things should be the same with training things, i.e, model and tokenizer!\n",
        "\n",
        "text = \"this is sina\"\n",
        "\n",
        "input = tokenizer(text, return_tensors = \"pt\")\n",
        "# we are using that return_tensor=\"pt\" to say that we need in the pytorch-aligned way.\n",
        "# we can use tf instead of pt, if we're using tensorflow as backend.\n",
        "\n",
        "output = model(**input)\n",
        "print(output)\n",
        "# it is gonna be a high dimentional vector."
      ],
      "metadata": {
        "id": "2kwZ8-LdkJMB",
        "outputId": "88e19ee2-4ccd-4bf8-8453-c1813ca460bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0735,  0.1171, -0.1137,  ..., -0.1849,  0.1637,  0.1906],\n",
            "         [-0.5274, -0.0381,  0.1222,  ..., -0.5427,  0.6652,  0.5235],\n",
            "         [-0.3611, -0.2636,  0.3259,  ..., -0.0176,  0.3163,  1.0792],\n",
            "         [ 0.3684, -0.7143,  0.2972,  ..., -0.0452,  0.1361, -0.1764],\n",
            "         [-0.3120, -0.4086, -0.1155,  ...,  0.2673,  0.3390,  0.0815],\n",
            "         [ 0.7924,  0.1487, -0.3462,  ..., -0.1642, -0.9591, -0.2393]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.1575e-01, -1.9694e-01,  4.8349e-01,  6.0623e-01, -3.2398e-01,\n",
            "         -5.4389e-02,  8.3786e-01,  2.0135e-01,  2.1983e-01, -9.9948e-01,\n",
            "          2.3199e-01,  7.7051e-02,  9.6494e-01, -2.3865e-01,  9.0038e-01,\n",
            "         -4.4461e-01,  1.7089e-02, -4.9665e-01,  3.0457e-01, -6.8125e-01,\n",
            "          4.6492e-01,  7.5874e-01,  6.4273e-01,  1.9611e-01,  2.8585e-01,\n",
            "          1.5133e-01, -4.3322e-01,  8.8309e-01,  9.2586e-01,  5.9573e-01,\n",
            "         -5.9266e-01,  1.0901e-01, -9.6698e-01, -9.4339e-02,  3.9820e-01,\n",
            "         -9.6584e-01,  1.1240e-01, -6.6689e-01,  3.8569e-02,  6.7694e-02,\n",
            "         -7.9427e-01,  1.4234e-01,  9.9214e-01, -3.8064e-01, -1.5139e-01,\n",
            "         -2.6843e-01, -9.9922e-01,  2.1116e-01, -7.9469e-01, -3.8640e-01,\n",
            "         -3.3756e-01, -5.7731e-01,  8.3718e-02,  2.8623e-01,  3.1156e-01,\n",
            "          3.4128e-01, -1.4152e-01,  1.3152e-01, -4.9625e-02, -4.4206e-01,\n",
            "         -5.1544e-01,  1.5529e-01,  1.2031e-01, -8.4517e-01, -3.9860e-01,\n",
            "         -4.6302e-01, -4.6301e-02, -1.2880e-01,  2.6854e-02, -1.0722e-01,\n",
            "          7.7017e-01,  1.3070e-01,  3.7943e-01, -6.9783e-01, -4.0537e-01,\n",
            "          1.1263e-01, -3.9050e-01,  9.9998e-01, -2.9168e-01, -9.4896e-01,\n",
            "         -5.0905e-01, -2.2832e-01,  3.1579e-01,  5.9032e-01, -4.7326e-01,\n",
            "         -9.9990e-01,  1.5825e-01, -7.9815e-02, -9.7392e-01,  1.1899e-01,\n",
            "          1.5861e-01, -1.0275e-01, -5.5047e-01,  3.0583e-01, -1.3089e-01,\n",
            "         -3.7218e-02, -1.9137e-01,  4.1711e-01, -7.2652e-02,  1.0544e-01,\n",
            "          1.7843e-02, -1.8282e-01,  1.6893e-03, -2.3356e-01,  3.2220e-02,\n",
            "         -2.8667e-01, -3.8727e-01,  1.6961e-01, -3.1269e-01,  5.1237e-01,\n",
            "          2.6387e-01, -2.1412e-01,  2.0006e-01, -9.2144e-01,  5.1292e-01,\n",
            "         -1.4453e-01, -9.5626e-01, -3.7635e-01, -9.6654e-01,  5.3425e-01,\n",
            "          1.0854e-01, -5.4413e-02,  9.3564e-01,  6.2977e-01,  1.4744e-01,\n",
            "          4.3480e-02,  3.8797e-01, -9.9999e-01, -1.7176e-01,  9.9473e-03,\n",
            "          2.2710e-01, -9.5447e-04, -9.4653e-01, -8.9061e-01,  4.6252e-01,\n",
            "          9.1812e-01,  5.3232e-02,  9.8749e-01, -1.7129e-01,  8.7755e-01,\n",
            "          1.4964e-01, -8.1924e-02, -3.9329e-01, -3.0310e-01,  2.3656e-01,\n",
            "          2.5752e-01, -5.4371e-01,  1.6266e-01,  1.5714e-01, -1.3049e-01,\n",
            "          1.0781e-01, -2.1968e-01,  3.6510e-01, -8.8925e-01, -3.0287e-01,\n",
            "          8.9609e-01,  2.6613e-01,  3.2482e-01,  6.3023e-01, -1.6632e-01,\n",
            "         -2.7912e-01,  7.2747e-01,  1.7429e-01,  2.4509e-01, -2.8695e-02,\n",
            "          3.2882e-01, -1.9160e-01,  3.2744e-01, -7.5963e-01,  2.7473e-01,\n",
            "          3.0673e-01, -9.3722e-02,  4.5355e-01, -9.4484e-01, -1.7181e-01,\n",
            "          3.2184e-01,  9.7048e-01,  6.4289e-01,  1.4707e-01, -6.7232e-02,\n",
            "         -1.5931e-01,  1.3633e-01, -8.8928e-01,  9.4139e-01, -9.8619e-02,\n",
            "          1.8310e-01,  5.5702e-01, -2.2031e-01, -7.7424e-01, -4.2493e-01,\n",
            "          7.5648e-01,  8.9353e-02, -7.4225e-01,  9.2790e-02, -3.7385e-01,\n",
            "         -2.9434e-01,  3.1478e-01,  2.8090e-01, -2.1888e-01, -3.5424e-01,\n",
            "          5.4535e-02,  8.5864e-01,  9.4089e-01,  6.7341e-01, -6.2201e-01,\n",
            "          4.1682e-01, -8.4744e-01, -2.6011e-01,  3.8767e-02,  1.8933e-01,\n",
            "          4.4307e-02,  9.8144e-01,  1.6525e-01, -6.7307e-02, -8.8784e-01,\n",
            "         -9.6491e-01, -3.8277e-02, -8.3295e-01,  5.0359e-02, -4.4921e-01,\n",
            "          1.6199e-01,  6.5132e-01, -1.4623e-01,  3.0673e-01, -9.6739e-01,\n",
            "         -7.0316e-01,  2.6402e-01, -1.1493e-01,  2.8667e-01, -1.2786e-01,\n",
            "          2.5622e-02, -2.5974e-01, -4.1426e-01,  6.7981e-01,  7.9898e-01,\n",
            "          4.8123e-01, -5.6131e-01,  7.6373e-01, -9.8524e-02,  8.0364e-01,\n",
            "         -4.7959e-01,  9.4046e-01, -1.8719e-01,  2.6027e-01, -8.7537e-01,\n",
            "          4.5392e-01, -8.4196e-01,  1.9684e-01, -1.9778e-02, -6.1627e-01,\n",
            "         -2.4510e-01,  3.5135e-01,  1.7123e-01,  7.9184e-01, -3.7536e-01,\n",
            "          9.8922e-01, -8.1198e-02, -9.0572e-01,  5.0491e-01,  7.3180e-03,\n",
            "         -9.6344e-01, -2.8723e-01,  1.5847e-01, -6.4240e-01, -2.4598e-01,\n",
            "         -3.3125e-01, -9.0024e-01,  7.9149e-01,  9.2046e-02,  9.6651e-01,\n",
            "          2.0418e-01, -8.5644e-01, -1.7097e-01, -8.0925e-01, -1.9843e-01,\n",
            "         -2.0192e-02,  5.9468e-01, -2.1207e-01, -9.2673e-01,  3.2986e-01,\n",
            "          4.1763e-01,  2.9143e-01,  6.0691e-01,  9.8814e-01,  9.8973e-01,\n",
            "          9.4191e-01,  8.3711e-01,  8.3541e-01, -4.9842e-01,  8.7478e-02,\n",
            "          9.9978e-01, -2.0820e-01, -9.9957e-01, -9.0242e-01, -4.4942e-01,\n",
            "          2.4547e-01, -9.9999e-01, -5.5813e-02,  8.4880e-02, -8.3101e-01,\n",
            "         -4.3167e-01,  9.5110e-01,  9.7420e-01, -9.9996e-01,  7.8308e-01,\n",
            "          8.9002e-01, -4.3068e-01,  4.7761e-02, -1.2781e-01,  9.4292e-01,\n",
            "          2.4039e-01,  2.2426e-01, -1.3825e-01,  1.7964e-01,  1.7109e-01,\n",
            "         -7.4831e-01,  4.5885e-01,  4.5475e-01,  1.2246e-01,  5.3744e-02,\n",
            "         -5.9033e-01, -8.7529e-01, -2.8342e-01, -1.7410e-03, -3.2297e-01,\n",
            "         -9.1760e-01, -7.6687e-02, -3.7283e-01,  4.8264e-01,  1.2372e-02,\n",
            "          9.7284e-02, -6.8821e-01,  1.3275e-01, -6.7477e-01,  2.7405e-01,\n",
            "          4.6448e-01, -8.8926e-01, -4.8400e-01, -1.0826e-02, -5.1418e-01,\n",
            "          3.7869e-01, -9.0016e-01,  9.3736e-01, -2.5875e-01, -1.1985e-01,\n",
            "          9.9997e-01, -3.8343e-01, -7.9393e-01,  2.6011e-01,  9.6694e-02,\n",
            "          1.5218e-02,  9.9994e-01,  2.4199e-01, -9.4625e-01, -3.4298e-01,\n",
            "          1.3054e-01, -2.6106e-01, -2.5741e-01,  9.9359e-01, -9.3244e-02,\n",
            "          4.4378e-01,  4.3829e-01,  9.2019e-01, -9.6969e-01, -6.9176e-02,\n",
            "         -8.5086e-01, -9.2690e-01,  9.2544e-01,  8.7348e-01,  3.7276e-02,\n",
            "         -4.8119e-01,  3.0070e-02,  6.9874e-02,  1.1211e-01, -9.3149e-01,\n",
            "          4.9555e-01,  3.5214e-01, -7.8685e-02,  8.6017e-01, -7.7172e-01,\n",
            "         -3.4164e-01,  3.1662e-01,  2.1309e-01,  4.2479e-01, -3.5915e-01,\n",
            "          3.7990e-01, -1.7946e-01,  3.2951e-02, -1.6366e-01,  2.0778e-01,\n",
            "         -9.4424e-01, -6.3020e-02,  9.9992e-01,  1.9549e-01, -5.9131e-01,\n",
            "         -6.1717e-02, -1.0706e-02, -3.4336e-01,  2.0897e-01,  3.1949e-01,\n",
            "         -2.1294e-01, -7.4189e-01, -2.4695e-01, -8.8405e-01, -9.6039e-01,\n",
            "          6.2927e-01,  1.3412e-01, -1.9121e-01,  9.9366e-01,  1.0787e-01,\n",
            "          8.4867e-02, -1.9867e-01, -1.7367e-01,  3.1414e-02,  4.5220e-01,\n",
            "         -4.8564e-01,  9.4309e-01, -1.5439e-01,  3.2495e-01,  7.3324e-01,\n",
            "          3.4161e-01, -2.1716e-01, -5.5689e-01, -1.5780e-02, -8.4811e-01,\n",
            "          5.7915e-02, -8.9896e-01,  9.2352e-01, -4.5981e-01,  2.3512e-01,\n",
            "          5.4397e-02, -1.7509e-01,  9.9996e-01,  4.4065e-01,  4.7116e-01,\n",
            "         -4.7337e-01,  8.0176e-01, -4.4775e-01, -6.7851e-01, -2.6985e-01,\n",
            "          1.0339e-01,  4.9137e-01, -1.6082e-01,  1.3660e-01, -9.2851e-01,\n",
            "         -3.6026e-01, -2.5118e-01, -9.5979e-01, -9.7694e-01,  5.9078e-01,\n",
            "          6.7245e-01,  4.0830e-02,  8.9009e-02, -5.2199e-01, -5.1251e-01,\n",
            "          9.5507e-02, -8.5153e-02, -8.7054e-01,  5.4456e-01, -1.7121e-01,\n",
            "          3.0447e-01, -1.6441e-01,  3.6739e-01, -4.6783e-01,  7.6805e-01,\n",
            "          6.2255e-01,  2.4208e-01,  6.6394e-03, -6.7975e-01,  6.4929e-01,\n",
            "         -7.2612e-01,  3.2002e-01, -9.1335e-02,  9.9999e-01, -3.3614e-01,\n",
            "         -3.5068e-01,  6.1816e-01,  6.2228e-01,  2.7363e-02,  1.2959e-01,\n",
            "         -3.1010e-01,  8.7643e-02,  4.4082e-01,  5.2204e-01, -7.0366e-01,\n",
            "         -1.8273e-01,  3.8417e-01, -6.0076e-01, -3.7175e-01,  6.4533e-01,\n",
            "         -9.2660e-02, -4.0373e-03,  4.4744e-02,  3.9374e-03,  9.9584e-01,\n",
            "         -7.9734e-02,  3.9629e-02, -3.8461e-01,  8.7233e-02, -1.8216e-01,\n",
            "         -4.6264e-01,  9.9968e-01,  2.8914e-01, -2.7270e-01, -9.7416e-01,\n",
            "          3.2538e-01, -8.5916e-01,  9.7028e-01,  6.1606e-01, -7.7028e-01,\n",
            "          3.3407e-01,  2.1305e-01, -8.1397e-02,  6.6828e-01, -1.1420e-01,\n",
            "         -2.0393e-01,  5.8554e-02,  9.4622e-02,  9.1752e-01, -2.6872e-01,\n",
            "         -9.0589e-01, -4.4229e-01,  2.2663e-01, -9.2075e-01,  6.3892e-01,\n",
            "         -3.9659e-01, -9.7003e-02, -1.4949e-01,  3.9901e-01,  7.2801e-01,\n",
            "         -7.3891e-02, -9.5119e-01, -9.3215e-02, -8.7752e-02,  9.2424e-01,\n",
            "          1.2990e-01, -3.5027e-01, -8.2932e-01, -5.4778e-01, -2.3080e-01,\n",
            "          4.7762e-01, -8.9806e-01,  9.3812e-01, -9.6641e-01,  2.3346e-01,\n",
            "          9.9973e-01,  1.7313e-01, -7.1279e-01,  7.0376e-02, -2.5905e-01,\n",
            "          1.2359e-01,  3.7191e-01,  4.4425e-01, -9.0963e-01, -1.4524e-01,\n",
            "         -4.6571e-02,  1.7866e-01, -9.1314e-02,  4.3314e-01,  5.6708e-01,\n",
            "          1.1190e-01, -2.9882e-01, -3.7859e-01, -5.8651e-02,  2.9267e-01,\n",
            "          6.0633e-01, -2.2281e-01, -5.2714e-02,  2.6499e-02, -4.5921e-02,\n",
            "         -8.4490e-01, -1.3855e-01, -1.0800e-01, -8.7239e-01,  4.9057e-01,\n",
            "         -9.9998e-01, -4.0375e-01, -5.1559e-01, -1.8588e-01,  7.2870e-01,\n",
            "         -8.4507e-02, -1.0865e-01, -6.0722e-01,  4.0400e-01,  8.2996e-01,\n",
            "          6.5970e-01, -8.0358e-02,  1.3378e-01, -5.6651e-01,  9.0678e-02,\n",
            "         -5.5410e-02,  1.5973e-01,  1.6013e-01,  6.5949e-01, -1.0117e-01,\n",
            "          9.9999e-01,  1.0939e-02, -3.7190e-01, -9.3653e-01,  1.7662e-01,\n",
            "         -1.2834e-01,  9.9521e-01, -8.5552e-01, -8.8644e-01,  1.8407e-01,\n",
            "         -2.9821e-01, -7.2454e-01,  1.3308e-01, -5.3520e-02, -4.6164e-01,\n",
            "          1.7960e-01,  9.3130e-01,  7.9472e-01, -3.3421e-01,  2.4943e-01,\n",
            "         -2.1627e-01, -3.0265e-01, -1.1285e-02, -4.6575e-01,  9.6401e-01,\n",
            "         -6.2146e-03,  8.1648e-01,  6.3649e-01,  1.8682e-01,  9.2685e-01,\n",
            "          8.4405e-02,  5.4633e-01,  6.2558e-02,  9.9978e-01,  1.9832e-01,\n",
            "         -8.5833e-01,  5.0522e-01, -9.6762e-01, -4.7403e-02, -9.0481e-01,\n",
            "          1.3867e-01,  1.9552e-02,  7.9351e-01, -1.3091e-01,  9.1231e-01,\n",
            "          4.9659e-01,  4.1647e-02,  2.1909e-01,  5.8760e-01,  2.4930e-01,\n",
            "         -8.4129e-01, -9.6675e-01, -9.6637e-01,  1.7172e-01, -3.4991e-01,\n",
            "         -4.5300e-02,  2.0051e-01,  1.1016e-01,  2.4566e-01,  2.5885e-01,\n",
            "         -9.9947e-01,  8.6903e-01,  2.5478e-01, -3.4979e-01,  9.1363e-01,\n",
            "          6.5106e-02,  8.9241e-02,  9.8198e-02, -9.6532e-01, -9.2608e-01,\n",
            "         -2.3540e-01, -2.2065e-01,  6.1035e-01,  4.9930e-01,  7.5671e-01,\n",
            "          2.1013e-01, -4.7202e-01, -1.3110e-02,  4.8255e-01, -2.6883e-02,\n",
            "         -9.7759e-01,  2.5151e-01,  2.7490e-01, -9.2435e-01,  9.1272e-01,\n",
            "         -4.7538e-01, -1.4243e-01,  5.9876e-01,  3.3534e-01,  8.7923e-01,\n",
            "          6.6588e-01,  3.2518e-01,  1.2683e-01,  3.2570e-01,  8.0313e-01,\n",
            "          9.0478e-01,  9.7223e-01,  3.2879e-01,  7.3246e-01,  3.2417e-01,\n",
            "          2.4107e-01,  1.3354e-01, -8.6540e-01,  4.3106e-02, -9.9763e-02,\n",
            "          3.2385e-02,  1.2925e-01, -1.0211e-01, -9.3158e-01,  3.2997e-01,\n",
            "         -9.5826e-02,  2.5122e-01, -3.1224e-01,  1.7483e-01, -2.8735e-01,\n",
            "         -1.1215e-01, -5.9602e-01, -2.5024e-01,  4.4647e-01,  2.5769e-01,\n",
            "          8.2516e-01,  8.2755e-02,  1.5941e-02, -5.2426e-01,  1.0164e-02,\n",
            "          3.8130e-01, -8.3794e-01,  8.5628e-01,  8.1567e-02,  4.2218e-01,\n",
            "         -4.0351e-01, -1.1205e-01,  4.2493e-01, -4.1015e-01, -2.3372e-01,\n",
            "         -1.7801e-01, -6.0357e-01,  7.8476e-01,  1.0034e-02, -3.5381e-01,\n",
            "         -2.9235e-01,  4.9914e-01,  2.3094e-01,  7.9525e-01,  3.0308e-01,\n",
            "          2.9147e-01,  8.7479e-02, -1.2701e-01,  2.2042e-01, -8.8792e-02,\n",
            "         -9.9953e-01,  3.2816e-01,  3.0192e-01, -2.5516e-01,  2.7535e-01,\n",
            "         -5.1952e-01,  6.7433e-02, -9.4414e-01, -1.6468e-02, -3.5270e-01,\n",
            "         -3.4098e-01, -4.2632e-01, -1.9469e-01,  3.2649e-01,  4.4544e-01,\n",
            "         -1.6815e-01,  8.1683e-01,  2.0430e-01,  6.2329e-01,  4.1811e-01,\n",
            "          4.0679e-01, -5.3481e-01,  8.3731e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) we can use another model, instead of transformers default model:**\n"
      ],
      "metadata": {
        "id": "Z7ZNsylqoADQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# we are proposing a new model.\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# we are using the same classifier as previous task.\n",
        "classifier = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokenizer)\n",
        "\n",
        "results = classifier([\"i'm somehow feel great yesterday but now is different\",\n",
        "                     \"there is nothing as good as this moment.\"])\n",
        "# we proposed 2 different sentences this time and it will return different lables for each of them.\n",
        "\n",
        "for i in results:\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "wOTBAYtVoJ71",
        "outputId": "e6747b74-5551-4b37-d185-ea1695e63a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9966962337493896}\n",
            "{'label': 'NEGATIVE', 'score': 0.9997376799583435}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) we can tokenize in some different ways:**"
      ],
      "metadata": {
        "id": "EEl-G0-hsJtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# now we are gonna tokenize\n",
        "tokens = tokenizer.tokenize(\"hi this is sina moradi from tehran and i'm learning transformers.\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tokenizer(\"hi this is sina moradi from tehran and i'm learning transformers.\")\n",
        "\n",
        "# now we are gonna print them seperately\n",
        "print(f' tokens: {tokens}')\n",
        "print(\"##########################################\")\n",
        "print(f' token_IDs: {token_ids}')\n",
        "print(\"##########################################\")\n",
        "print(f' input_IDs: {input_ids}')\n",
        "\n",
        "\n",
        "# we can also make it done in 2 sentences with different sizes.\n",
        "# padding is printing \"0\" for empty [arts of the shorter sentence.\n",
        "# max length determines the point to cut the sentence, cosidering its size.\n",
        "x_train = [\"i'm so happy to learn this module its so amazing\",\n",
        "           \"i hope to continue in the next hours\"]\n",
        "\n",
        "# in the train phase, we are gonna put the texts in a batch, instead of putting them seperately.\n",
        "# batchs are matrices with specific rows and columns.\n",
        "\n",
        "batch = tokenizer(x_train, padding = True,\n",
        "                  truncation = True,\n",
        "                  max_length = 512,\n",
        "                  return_tensors = \"pt\")\n",
        "print(\"##########################################\")\n",
        "print(batch)\n"
      ],
      "metadata": {
        "id": "_T2vHbFgsLoT",
        "outputId": "38668467-3f29-4815-c66a-57296de223d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tokens: ['hi', 'this', 'is', 'sin', '##a', 'mora', '##di', 'from', 'tehran', 'and', 'i', \"'\", 'm', 'learning', 'transformers', '.']\n",
            "##########################################\n",
            " token_IDs: [7632, 2023, 2003, 8254, 2050, 26821, 4305, 2013, 13503, 1998, 1045, 1005, 1049, 4083, 19081, 1012]\n",
            "##########################################\n",
            " input_IDs: {'input_ids': [101, 7632, 2023, 2003, 8254, 2050, 26821, 4305, 2013, 13503, 1998, 1045, 1005, 1049, 4083, 19081, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "##########################################\n",
            "{'input_ids': tensor([[  101,  1045,  1005,  1049,  2061,  3407,  2000,  4553,  2023, 11336,\n",
            "          2049,  2061,  6429,   102],\n",
            "        [  101,  1045,  3246,  2000,  3613,  1999,  1996,  2279,  2847,   102,\n",
            "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) we can work directly with pytorch for better options:**"
      ],
      "metadata": {
        "id": "O_WQ-Qn_x1ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "x_train = [\"i'm so happy to learn this module its so amazing\",\n",
        "           \"i hope to continue in the next hours\"]\n",
        "batch = tokenizer(x_train, padding = True,\n",
        "                  truncation = True,\n",
        "                  max_length = 512,\n",
        "                  return_tensors = \"pt\")\n",
        "print(batch)\n",
        "print(\"##########################################\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**batch)\n",
        "  print(outputs)\n",
        "  print(\"##########################################\")\n",
        "  predictions = F.softmax(outputs.logits, dim = 1)\n",
        "  print(predictions)\n",
        "  print(\"##########################################\")\n",
        "  labels = torch.argmax(predictions, dim = 1)\n",
        "  print(labels)\n",
        "  print(\"##########################################\")\n",
        "  labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
        "  print(labels)\n"
      ],
      "metadata": {
        "id": "tM_vvfj3x2Wu",
        "outputId": "f52cb9be-4b43-48f1-e95c-3ea66b1724e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  1049,  2061,  3407,  2000,  4553,  2023, 11336,\n",
            "          2049,  2061,  6429,   102],\n",
            "        [  101,  1045,  3246,  2000,  3613,  1999,  1996,  2279,  2847,   102,\n",
            "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n",
            "##########################################\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3352,  4.6670],\n",
            "        [-2.1270,  2.1375]]), hidden_states=None, attentions=None)\n",
            "##########################################\n",
            "tensor([[1.2311e-04, 9.9988e-01],\n",
            "        [1.3863e-02, 9.8614e-01]])\n",
            "##########################################\n",
            "tensor([1, 1])\n",
            "##########################################\n",
            "['POSITIVE', 'POSITIVE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9AW3lJq0G6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}